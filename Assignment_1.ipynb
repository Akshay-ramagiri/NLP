{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOukHOThgHT7MQmkNsP3aEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshay-ramagiri/NLP/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4Q. Find stop words and BOW from the given paragraph."
      ],
      "metadata": {
        "id": "cSfmS4esMxSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzkNizWqSCHF",
        "outputId": "1197563d-57e7-47d6-9e3e-3556018c6e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxLIbw7bMkQK",
        "outputId": "f455c0e4-6dcb-4733-e689-01a7de7656d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words in the given paragraph are :  ['is', 'of', 'the', 'and', 'most', 'for', 'and', 'you', 'can', 'the', 'of', 'in', 'the', 'from', 'a', 'you', 'can', 'your', 'into', 'and', 'then', 'the', 'if', 'it', 'in', 'the', 'of', 'by']\n",
            "Bag of Words in the given paragraph are :  ['The', 'NLTK', 'library', 'one', 'oldest', 'commonly', 'used', 'Python', 'libraries', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'stop', 'words', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'sentence', ',', 'divide', 'text', 'words', 'remove', 'word', 'exits', 'list', 'stop', 'words', 'provided', 'NLTK', '.']\n"
          ]
        }
      ],
      "source": [
        "##TO FIND STOP WORDS AND BAG OF WORDS (BOW) IN THE GIVEN PARAGRAPH.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "paragraph = \"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "stopWords = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(paragraph)\n",
        "Stop_Words = []\n",
        "Bag_Of_Words = []\n",
        "\n",
        "for words in tokens:\n",
        "    if words in stopWords:\n",
        "        Stop_Words.append(words)\n",
        "\n",
        "for words in tokens:\n",
        "    if words not in stopWords:\n",
        "        Bag_Of_Words.append(words)\n",
        "\n",
        "print(\"Stop words in the given paragraph are : \", Stop_Words)\n",
        "print(\"Bag of Words in the given paragraph are : \",Bag_Of_Words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5Q. From the above paragraph print frequency of each word using NLTK?"
      ],
      "metadata": {
        "id": "zZ9PT3tDT1kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##TO FIND FREQUENCY OF EACH WORD IN TGE GIVEN PARAGRAPH.\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "paragraph = \"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "tokens = word_tokenize(paragraph)\n",
        "data_analysis = nltk.FreqDist(tokens)\n",
        "filter_words = dict([(m, n) for m, n in data_analysis.items()])\n",
        " \n",
        "print(\"WORD : FREQUENCY\\n\")\n",
        "for key in sorted(filter_words):\n",
        "    print(\"{} : {}\".format(key, filter_words[key]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMtmkfiNUFX0",
        "outputId": "2cfb37c0-13cc-458c-ecce-f8172723c6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD : FREQUENCY\n",
            "\n",
            ", : 2\n",
            ". : 3\n",
            "Language : 1\n",
            "NLTK : 3\n",
            "Natural : 1\n",
            "Processing : 1\n",
            "Python : 1\n",
            "The : 1\n",
            "To : 1\n",
            "a : 1\n",
            "and : 3\n",
            "by : 1\n",
            "can : 2\n",
            "commonly : 1\n",
            "corpus : 1\n",
            "divide : 1\n",
            "exits : 1\n",
            "find : 1\n",
            "for : 1\n",
            "from : 1\n",
            "if : 1\n",
            "in : 2\n",
            "into : 1\n",
            "is : 1\n",
            "it : 1\n",
            "libraries : 1\n",
            "library : 1\n",
            "list : 2\n",
            "module : 1\n",
            "most : 1\n",
            "of : 3\n",
            "oldest : 1\n",
            "one : 1\n",
            "provided : 1\n",
            "removal : 1\n",
            "remove : 2\n",
            "sentence : 1\n",
            "stop : 4\n",
            "supports : 1\n",
            "text : 1\n",
            "the : 5\n",
            "then : 1\n",
            "used : 1\n",
            "word : 2\n",
            "words : 4\n",
            "you : 2\n",
            "your : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8Q. Find BoW for the given paragraph? And also find stem and lemma words?"
      ],
      "metadata": {
        "id": "BwzK2FKVYwho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##TO FIND BAG OF WORDS (BOW) IN THE GIVEN PARAGRAPH.\n",
        "\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "stopWords = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(paragraph)\n",
        "Bag_Of_Words = []\n",
        "\n",
        "for words in tokens:\n",
        "    if words not in stopWords:\n",
        "        Bag_Of_Words.append(words)\n",
        "\n",
        "print(\"Bag of Words in the given paragraph are : \",Bag_Of_Words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2ItRG9IZOPD",
        "outputId": "f1924561-579c-4473-c179-8cb74ccdb315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words in the given paragraph are :  ['Text', 'Summarization', 'one', 'applications', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'bound', 'huge', 'impact', 'lives', '.', 'With', 'growing', 'digital', 'media', 'ever-growing', 'publishing', '–', 'time', 'go', 'entire', 'articles', '/', 'documents', '/', 'books', 'decide', 'whether', 'useful', '?', 'Thankfully', '–', 'technology', 'already', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO FIND STEM WORDS.\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        " \n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "for words in tokens:\n",
        "    print(\"Stem for {} is {}\".format(words, ps.stem(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSQIaf-LcRHg",
        "outputId": "6dad99b6-2785-416e-cf1c-515fa6f9dd6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem for Text is text\n",
            "Stem for Summarization is summar\n",
            "Stem for is is is\n",
            "Stem for one is one\n",
            "Stem for of is of\n",
            "Stem for those is those\n",
            "Stem for applications is applic\n",
            "Stem for of is of\n",
            "Stem for Natural is natur\n",
            "Stem for Language is languag\n",
            "Stem for Processing is process\n",
            "Stem for ( is (\n",
            "Stem for NLP is nlp\n",
            "Stem for ) is )\n",
            "Stem for which is which\n",
            "Stem for is is is\n",
            "Stem for bound is bound\n",
            "Stem for to is to\n",
            "Stem for have is have\n",
            "Stem for a is a\n",
            "Stem for huge is huge\n",
            "Stem for impact is impact\n",
            "Stem for on is on\n",
            "Stem for our is our\n",
            "Stem for lives is live\n",
            "Stem for . is .\n",
            "Stem for With is with\n",
            "Stem for growing is grow\n",
            "Stem for digital is digit\n",
            "Stem for media is media\n",
            "Stem for and is and\n",
            "Stem for ever-growing is ever-grow\n",
            "Stem for publishing is publish\n",
            "Stem for – is –\n",
            "Stem for who is who\n",
            "Stem for has is ha\n",
            "Stem for the is the\n",
            "Stem for time is time\n",
            "Stem for to is to\n",
            "Stem for go is go\n",
            "Stem for through is through\n",
            "Stem for entire is entir\n",
            "Stem for articles is articl\n",
            "Stem for / is /\n",
            "Stem for documents is document\n",
            "Stem for / is /\n",
            "Stem for books is book\n",
            "Stem for to is to\n",
            "Stem for decide is decid\n",
            "Stem for whether is whether\n",
            "Stem for they is they\n",
            "Stem for are is are\n",
            "Stem for useful is use\n",
            "Stem for or is or\n",
            "Stem for not is not\n",
            "Stem for ? is ?\n",
            "Stem for Thankfully is thank\n",
            "Stem for – is –\n",
            "Stem for this is thi\n",
            "Stem for technology is technolog\n",
            "Stem for is is is\n",
            "Stem for already is alreadi\n",
            "Stem for here is here\n",
            "Stem for . is .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##TO FIND LEMMA WORDS.\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "tokens = nltk.word_tokenize(paragraph)\n",
        "\n",
        "for words in tokens:\n",
        "    print(\"Lemma for {} is {}\".format(words, wordnet_lemmatizer.lemmatize(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAyyhlTweAZb",
        "outputId": "56533d81-d86c-4db3-c41c-3c36bd538519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for Text is Text\n",
            "Lemma for Summarization is Summarization\n",
            "Lemma for is is is\n",
            "Lemma for one is one\n",
            "Lemma for of is of\n",
            "Lemma for those is those\n",
            "Lemma for applications is application\n",
            "Lemma for of is of\n",
            "Lemma for Natural is Natural\n",
            "Lemma for Language is Language\n",
            "Lemma for Processing is Processing\n",
            "Lemma for ( is (\n",
            "Lemma for NLP is NLP\n",
            "Lemma for ) is )\n",
            "Lemma for which is which\n",
            "Lemma for is is is\n",
            "Lemma for bound is bound\n",
            "Lemma for to is to\n",
            "Lemma for have is have\n",
            "Lemma for a is a\n",
            "Lemma for huge is huge\n",
            "Lemma for impact is impact\n",
            "Lemma for on is on\n",
            "Lemma for our is our\n",
            "Lemma for lives is life\n",
            "Lemma for . is .\n",
            "Lemma for With is With\n",
            "Lemma for growing is growing\n",
            "Lemma for digital is digital\n",
            "Lemma for media is medium\n",
            "Lemma for and is and\n",
            "Lemma for ever-growing is ever-growing\n",
            "Lemma for publishing is publishing\n",
            "Lemma for – is –\n",
            "Lemma for who is who\n",
            "Lemma for has is ha\n",
            "Lemma for the is the\n",
            "Lemma for time is time\n",
            "Lemma for to is to\n",
            "Lemma for go is go\n",
            "Lemma for through is through\n",
            "Lemma for entire is entire\n",
            "Lemma for articles is article\n",
            "Lemma for / is /\n",
            "Lemma for documents is document\n",
            "Lemma for / is /\n",
            "Lemma for books is book\n",
            "Lemma for to is to\n",
            "Lemma for decide is decide\n",
            "Lemma for whether is whether\n",
            "Lemma for they is they\n",
            "Lemma for are is are\n",
            "Lemma for useful is useful\n",
            "Lemma for or is or\n",
            "Lemma for not is not\n",
            "Lemma for ? is ?\n",
            "Lemma for Thankfully is Thankfully\n",
            "Lemma for – is –\n",
            "Lemma for this is this\n",
            "Lemma for technology is technology\n",
            "Lemma for is is is\n",
            "Lemma for already is already\n",
            "Lemma for here is here\n",
            "Lemma for . is .\n"
          ]
        }
      ]
    }
  ]
}